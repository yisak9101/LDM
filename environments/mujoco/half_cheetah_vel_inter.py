import random

import numpy as np

from .half_cheetah import HalfCheetahEnv


class HalfCheetahVelInterEnv(HalfCheetahEnv):
    """Half-cheetah environment with target velocity, as described in [1]. The
    code is adapted from
    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/rllab/envs/mujoco/half_cheetah_env_rand.py

    The half-cheetah follows the dynamics from MuJoCo [2], and receives at each
    time step a reward composed of a control cost and a penalty equal to the
    difference between its current velocity and the target velocity. The tasks
    are generated by sampling the target velocities from the uniform
    distribution on [0, 2].

    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic
        Meta-Learning for Fast Adaptation of Deep Networks", 2017
        (https://arxiv.org/abs/1703.03400)
    [2] Emanuel Todorov, Tom Erez, Yuval Tassa, "MuJoCo: A physics engine for
        model-based control", 2012
        (https://homes.cs.washington.edu/~todorov/papers/TodorovIROS12.pdf)
    """

    def __init__(self, max_episode_steps=200):
        self.set_task(self.sample_tasks(1)[0])
        self._max_episode_steps = max_episode_steps
        self.task_dim = 1

        eval_for_train_task_list = [0.25, 3.25]  # [:2]
        eval_for_indis_task_list = [0.25, 3.25]  # [2:4]
        eval_for_test_task_list = [0.75, 1.25, 1.75, 2.25, 2.75]  # [4:9]

        train_tsne_tasks = [0.1, 0.2, 0.3, 0.4, 0.5] + [3.0, 3.1, 3.2, 3.3, 3.4, 3.5]  # 11개
        test_tsne_tasks = [0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9]  # 24개

        # 총 44개
        self.eval_task_list = eval_for_train_task_list + \
                              eval_for_indis_task_list + \
                              eval_for_test_task_list + \
                              train_tsne_tasks + test_tsne_tasks  #

        super(HalfCheetahVelInterEnv, self).__init__()

    def step(self, action):
        xposbefore = self.sim.data.qpos[0]
        self.do_simulation(action, self.frame_skip)
        xposafter = self.sim.data.qpos[0]

        forward_vel = (xposafter - xposbefore) / self.dt
        forward_reward = -1.0 * abs(forward_vel - self.goal_velocity)
        ctrl_cost = 0.5 * 1e-1 * np.sum(np.square(action))

        observation = self._get_obs()
        reward = forward_reward - ctrl_cost
        done = False
        infos = dict(reward_forward=forward_reward,
                     reward_ctrl=-ctrl_cost,
                     task=self.get_task())
        return observation, reward, done, infos

    def set_task(self, task):
        self.goal_velocity = task

    def get_task(self):
        return self.goal_velocity

    def sample_tasks(self, n_tasks):
        oracle=False
        if oracle:
            return [random.uniform(0.0, 3.5) for _ in range(n_tasks)]
        else:
            vel_rand = 2.0 * random.random()
            if vel_rand <1.0:
                return [random.uniform(0.0, 0.5) for _ in range(n_tasks)]
            else:
                return [random.uniform(3.0, 3.5) for _ in range(n_tasks)]

    def get_test_task_list(self):
        return self.eval_task_list

    def reset_task(self, task):
        if task is None:
            task = self.sample_tasks(1)[0]
            self.set_task(task)
        else:
            self.set_task(self.get_test_task_list()[task])
        # self.reset()


# class HalfCheetahRandVelOracleEnv(HalfCheetahVelEnv):

#     def _get_obs(self):
#         return np.concatenate([
#             self.sim.data.qpos.flat[1:],
#             self.sim.data.qvel.flat,
#             self.get_body_com("torso").flat,
#             [self.goal_velocity]
#         ])
